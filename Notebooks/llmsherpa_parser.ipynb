{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LLMSherpa Smart Document Parser\n",
    "\n",
    "### Introduction to `nlm-ingestor`\n",
    "\n",
    "The `nlm-ingestor` repository provides a powerful and efficient parsing solution tailored for Retrieval-Augmented Generation (RAG) systems. Previously a proprietary tool, also exclusively available to Microsoft Azure users, this parser is now open-source, making it accessible for broader use. \n",
    "\n",
    "- Recommended for accuracy and speed and as RAG-friendly, the `nlm-ingestor` \n",
    "- An advanced rule-based and model-based techniques to handle various document formats, including PDF, HTML, and DOCX. optimizing processing of text-heavy documents or complex layouts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "To run `nlm-ingestor` locally using Docker, follow these steps:\n",
    "\n",
    "1. **Pull the Docker Image**\n",
    "\n",
    "   Download the latest Docker image:\n",
    "\n",
    "   ```bash\n",
    "   docker pull ghcr.io/nlmatics/nlm-ingestor:latest\n",
    "   ```\n",
    "\n",
    "2. **Run the Docker Container**\n",
    "\n",
    "   Start the Docker container and map the container's port to a port on your local machine:\n",
    "\n",
    "   ```bash\n",
    "   docker run -p 5010:5001 ghcr.io/nlmatics/nlm-ingestor:latest\n",
    "   ```\n",
    "\n",
    "\n",
    "3. **Clone the `llmsherpa` Repository**\n",
    "\n",
    "   Download the `llmsherpa` repository from GitHub:\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/nlmatics/llmsherpa.git\n",
    "   ```\n",
    "\n",
    "   Note the path to the `llmsherpa` directory after cloning. For example, if you cloned it to your home directory, the path might be `~/llmsherpa`.\n",
    "\n",
    "4. **Set Up Environment Variables**\n",
    "\n",
    "   Create a `.env` file in your project directory with the following content, replacing the path with your `llmsherpa` directory path:\n",
    "\n",
    "   PATH_TO_LLMSHERPA=~/llmsherpa\n",
    "\n",
    "5. **Verify the Configuration**\n",
    "\n",
    "   ```bash\n",
    "   > curl -I http://localhost:5010/   \n",
    "\n",
    "   This should print:\n",
    "\n",
    "   HTTP/1.1 200 OK\n",
    "   Server: Werkzeug/3.0.3 Python/3.11.9\n",
    "   Date: Thu, 19 Sep 2024 22:47:35 GMT\n",
    "   Content-Type: text/html; charset=utf-8\n",
    "   Content-Length: 18\n",
    "   Connection: close\n",
    "   ```\n",
    "\n",
    "To check that the environment variable is set correctly, you can run the following Python code in a Jupyter cell:\n",
    "\n",
    "\n",
    "(TODO: Automate this step by creating a Docker container to handle the setup process.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "import os\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llmsherpa_path = os.getenv('PATH_TO_LLMSHERPA')\n",
    "\n",
    "if llmsherpa_path:\n",
    "    sys.path.insert(0, llmsherpa_path)\n",
    "    print(f\"Added {llmsherpa_path} to sys.path\")\n",
    "else:\n",
    "    print(\"Environment variable PATH_TO_LLMSHERPA is not set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse a Single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc = pdf_reader.read_pdf(pdf_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc` object contains chunks of the document extracted based on the logical structure of document such as paragraph, table, list etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llmsherpa.readers.layout_reader.Paragraph object at 0x7f92abfbd910>\n",
      "<llmsherpa.readers.layout_reader.Paragraph object at 0x7f92abfbd9a0>\n",
      "<llmsherpa.readers.layout_reader.Paragraph object at 0x7f92abfbda90>\n",
      "<llmsherpa.readers.layout_reader.Paragraph object at 0x7f92abfbdb50>\n",
      "<llmsherpa.readers.layout_reader.Paragraph object at 0x7f92abfbdb80>\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.chunks()[:5]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access to chunks based on the type and display in HTML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><th><td colSpan=1></td><td colSpan=1>SQuAD 1.1 EM/F1</td><td colSpan=1>SQuAD 2.0 EM/F1</td><td colSpan=1>MNLI m/mm</td><td colSpan=1>SST Acc</td><td colSpan=1>QQP Acc</td><td colSpan=1>QNLI Acc</td><td colSpan=1>STS-B Acc</td><td colSpan=1>RTE Acc</td><td colSpan=1>MRPC Acc</td><td colSpan=1>CoLA Mcc</td></th><tr><td colSpan=1>BERT</td><td colSpan=1>84.1/90.9</td><td colSpan=1>79.0/81.8</td><td colSpan=1>86.6/-</td><td colSpan=1>93.2</td><td colSpan=1>91.3</td><td colSpan=1>92.3</td><td colSpan=1>90.0</td><td colSpan=1>70.4</td><td colSpan=1>88.0</td><td colSpan=1>60.6</td></tr><tr><td colSpan=1>UniLM</td><td colSpan=1>-/-</td><td colSpan=1>80.5/83.4</td><td colSpan=1>87.0/85.9</td><td colSpan=1>94.5</td><td colSpan=1>-</td><td colSpan=1>92.7</td><td colSpan=1>-</td><td colSpan=1>70.9</td><td colSpan=1>-</td><td colSpan=1>61.1</td></tr><tr><td colSpan=1>XLNet</td><td colSpan=1>89.0/94.5</td><td colSpan=1>86.1/88.8</td><td colSpan=1>89.8/-</td><td colSpan=1>95.6</td><td colSpan=1>91.8</td><td colSpan=1>93.9</td><td colSpan=1>91.8</td><td colSpan=1>83.8</td><td colSpan=1>89.2</td><td colSpan=1>63.6</td></tr><tr><td colSpan=1>RoBERTa</td><td colSpan=1>88.9/94.6</td><td colSpan=1>86.5/89.4</td><td colSpan=1>90.2/90.2</td><td colSpan=1>96.4</td><td colSpan=1>92.2</td><td colSpan=1>94.7</td><td colSpan=1>92.4</td><td colSpan=1>86.6</td><td colSpan=1>90.9</td><td colSpan=1>68.0</td></tr><tr><td colSpan=1>BART</td><td colSpan=1>88.8/94.6</td><td colSpan=1>86.1/89.2</td><td colSpan=1>89.9/90.1</td><td colSpan=1>96.6</td><td colSpan=1>92.5</td><td colSpan=1>94.9</td><td colSpan=1>91.2</td><td colSpan=1>87.0</td><td colSpan=1>90.4</td><td colSpan=1>62.8</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "# Access to 5th table in the document and print in html format\n",
    "HTML(doc.tables()[5].to_html()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bbox': [179.52, 165.33, 421.01, 177.29000000000002],\n",
       " 'block_class': 'cls_5',\n",
       " 'block_idx': 2,\n",
       " 'level': 1,\n",
       " 'page_idx': 0,\n",
       " 'sentences': ['{mikelewis,yinhanliu,naman}@fb.com'],\n",
       " 'tag': 'header'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the 1st section in JSON format\n",
    "doc.sections()[1].block_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sections()[0].to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML(doc.to_html())\n",
    "# HTML(doc.sections()[0].to_html(include_children=True, recurse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Folder containing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"\n",
    "    A class to represent a document with metadata and content \n",
    "    We can use this metadata/hierarchy information in retrieval & search for RAG\n",
    "\n",
    "    Attributes:\n",
    "        metadata (dict): A dictionary containing metadata about the document.\n",
    "        page_content (str): The content of the document.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metadata: dict, page_content: str):\n",
    "        \"\"\"\n",
    "        Initialize a Document instance.\n",
    "\n",
    "        Args:\n",
    "            metadata (dict): A dictionary containing metadata about the document.\n",
    "            page_content (str): The content of the document.\n",
    "        \"\"\"\n",
    "        self.metadata = metadata\n",
    "        self.page_content = page_content\n",
    "\n",
    "    def format_metadata(self):\n",
    "        \"\"\"\n",
    "        Format the metadata of the document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing formatted metadata with predefined attributes.\n",
    "        \"\"\"\n",
    "        # Define attributes based on observed attributes in chunks\n",
    "        attributes = ['source', 'page_idx', 'block_idx', 'tag', 'type']\n",
    "        # Extract relevant metadata, defaulting to 'unknown' if not present\n",
    "        formatted_metadata = {key: self.metadata.get(key, 'unknown') for key in attributes}\n",
    "        return formatted_metadata\n",
    "    \n",
    "    @property\n",
    "    def source(self):\n",
    "        return self.metadata.get('source', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def page_idx(self):\n",
    "        return self.metadata.get('page_idx', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def block_idx(self):\n",
    "        return self.metadata.get('block_idx', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def tag(self):\n",
    "        return self.metadata.get('tag', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self.metadata.get('type', 'unknown')\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the Document instance.\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted string containing metadata and truncated content.\n",
    "        \"\"\"\n",
    "        # Create a formatted string from metadata and truncate content\n",
    "        metadata_str = ', '.join(f\"{key}={value}\" for key, value in self.format_metadata().items())\n",
    "        return f\"Document({metadata_str}, content={self.page_content})\"\n",
    "\n",
    "def process_pdf(file_path: str, pdf_reader: LayoutPDFReader) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process a PDF file and extract its content into Document instances.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file.\n",
    "        pdf_reader (LayoutPDFReader): An instance of LayoutPDFReader to read the PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document instances representing the content of the PDF.\n",
    "    \"\"\"\n",
    "    # Read PDF content\n",
    "    doc = pdf_reader.read_pdf(file_path)\n",
    "\n",
    "    documents = []\n",
    "    for chunk in doc.chunks():\n",
    "        # Extract metadata\n",
    "        page_idx = chunk.page_idx\n",
    "        block_idx = chunk.block_idx\n",
    "        tag = chunk.tag\n",
    "        chunk_type = type(chunk).__name__  # Get the type of the chunk (e.g., 'Paragraph', 'Table', etc.)\n",
    "        page_content = chunk.to_text()\n",
    "\n",
    "        # Convert to Document format\n",
    "        metadata = {\n",
    "            'source': os.path.basename(file_path),\n",
    "            'page_idx': page_idx,\n",
    "            'block_idx': block_idx,\n",
    "            'tag': tag,\n",
    "            'type': chunk_type\n",
    "        }\n",
    "        document = Document(metadata=metadata, page_content=page_content)\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def process_folder(folder_path: str, pdf_reader: LayoutPDFReader):\n",
    "    \"\"\"\n",
    "    Process all PDF files in a folder and extract their content into Document instances.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing PDF files.\n",
    "        pdf_reader (LayoutPDFReader): An instance of LayoutPDFReader to read the PDFs.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document instances representing the content of all PDFs in the folder.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            # Process each PDF file\n",
    "            documents = process_pdf(file_path, pdf_reader)\n",
    "            all_documents.extend(documents)\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ShowcaseUserGuide.pdf\n",
      "Processing file: access_019-access-management-system-user-guide-v4-0.pdf\n",
      "Processing file: nihms-1769170.pdf\n",
      "Processing file: Bookshelf_NBK5295.pdf\n",
      "{'source': 'ShowcaseUserGuide.pdf', 'page_idx': 0, 'block_idx': 3, 'tag': 'para', 'type': 'Paragraph'}\n",
      "UK Biobank holds an unprecedented amount of data on half a million participants aged 40-69 years (with a roughly even number of men and women) recruited between 2006 and 2010 throughout the UK.\n",
      "Showcase (available at http://www.ukbiobank.ac.uk) aims to present the data available for health-related research in a comprehensive and concise way, and to provide technical information for researchers considering applying to use the resource.\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USE\n",
    "# llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\"\n",
    "# pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "\n",
    "folder_path = \"../data\"  # Path to your folder containing PDFs\n",
    "all_docs = process_folder(folder_path, pdf_reader)\n",
    "\n",
    "# Output the first document for verification\n",
    "print(all_docs[0].metadata)\n",
    "print(all_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Processed 200-300 pages in 4 documents in 7.3 seconds, might be improved with async.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=3, tag=para, type=Paragraph, content=UK Biobank holds an unprecedented amount of data on half a million participants aged 40-69 years (with a roughly even number of men and women) recruited between 2006 and 2010 throughout the UK.\n",
       " Showcase (available at http://www.ukbiobank.ac.uk) aims to present the data available for health-related research in a comprehensive and concise way, and to provide technical information for researchers considering applying to use the resource.),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=4, tag=para, type=Paragraph, content=This user guide is designed to give you an overview of the data and provides some instructions on how to navigate your way through the system.),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=6, tag=list_item, type=ListItem, content=• Have a printout of this user guide handy when you first use Showcase),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=7, tag=list_item, type=ListItem, content=• Read the background information about UK Biobank and details on access procedures at http://www.ukbiobank.ac.uk.),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=8, tag=list_item, type=ListItem, content=• Take time to familiarise yourself with the Showcase structure, the accompanying documentation and the descriptions provided for each data-field before completing a preliminary application to use the resource.),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=9, tag=list_item, type=ListItem, content=• While it is worth checking what variables are available in the Showcase Resource, there is no requirement to construct a Showcase basket until the Main Application stage.),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=10, tag=list_item, type=ListItem, content=• Note that Showcase is continually under development, as new data on exposure and health outcomes is incorporated into the database.\n",
       " More information on the timelines for future data is availability in the Essential information section of data Showcase.),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=11, tag=para, type=Paragraph, content=If you encounter problems or faults, please email showcase@ukbiobank.ac.uk),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=1, block_idx=14, tag=para, type=Paragraph, content=All participants in UK Biobank were recruited through assessment centres, designed specifically for this purpose (a map of the 22 assessment centres is provided in the Essential Information section of the Showcase).\n",
       " Data collected at the assessment visit included information on a participant’s health and lifestyle, hearing and cognitive function, collected through a touchscreen questionnaire and brief verbal interview.\n",
       " A range of physical measurements were also performed, which included: blood pressure; arterial stiffness; eye measures (visual acuity, refractometry, intraocular pressure, optical coherence tomography); body composition measures (including impedance); hand-grip strength; ultrasound bone densitometry; spirometry; and an exercise/fitness test with ECG.\n",
       " Samples of blood, urine and saliva were also collected.\n",
       " Some of these measures were incorporated into the Assessment visit towards the end of the recruitment period and are therefore not available for all 500,000 participants (see the ‘Essential Information’ section of the Showcase for more information on timelines).),\n",
       " Document(source=ShowcaseUserGuide.pdf, page_idx=1, block_idx=15, tag=para, type=Paragraph, content=During 2006, over 3,000 participants were included in the pilot phase of recruitment.\n",
       " Where possible, data collected from the pilot and the main recruitment phases have been combined.\n",
       " Where modifications to the protocol were made after the pilot study, the data-fields from the pilot and main recruitment phase are listed separately (e.g., touchscreen questions on medications, family history, qualifications and household income).\n",
       " Pilot data-fields can be identified easily; they include ‘pilot’ in the data-field name and have a field ID number in the 10000s).\n",
       " In addition, cognitive function tests that were felt to be too time-consuming and/or relatively uninformative were omitted from the main phase of recruitment (i.e. the lights pattern memory test’ on the touchscreen questionnaire and the ‘word test’ that was performed during the verbal interview stage).\n",
       " A web-based dietary questionnaire was included in the Assessment visit towards the end of the recruitment period, and participants were also invited via e-mail to complete the questionnaire on four further occasions (between 2011 and 2012).)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(source=ShowcaseUserGuide.pdf, page_idx=0, block_idx=3, tag=para, type=Paragraph, content=UK Biobank holds an unprecedented amount of data on half a million participants aged 40-69 years (with a roughly even number of men and women) recruited between 2006 and 2010 throughout the UK.\n",
       "Showcase (available at http://www.ukbiobank.ac.uk) aims to present the data available for health-related research in a comprehensive and concise way, and to provide technical information for researchers considering applying to use the resource.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShowcaseUserGuide.pdf\n",
      "0\n",
      "3\n",
      "para\n",
      "Paragraph\n",
      "UK Biobank holds an unprecedented amount of data on half a million participants aged 40-69 years (with a roughly even number of men and women) recruited between 2006 and 2010 throughout the UK.\n",
      "Showcase (available at http://www.ukbiobank.ac.uk) aims to present the data available for health-related research in a comprehensive and concise way, and to provide technical information for researchers considering applying to use the resource.\n"
     ]
    }
   ],
   "source": [
    "doc = all_docs[0]\n",
    "print(doc.source)      # Access source\n",
    "print(doc.page_idx)    # Access page index\n",
    "print(doc.block_idx)   # Access block index\n",
    "print(doc.tag)         # Access tag\n",
    "print(doc.type)        # Access type\n",
    "print(doc.page_content)  # Access content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "- https://github.com/nlmatics/llmsherpa\n",
    "- https://github.com/nlmatics/nlm-ingestor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperrag-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
