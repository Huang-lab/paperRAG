{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document as BaseDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Any, Dict, Literal, List, Union\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level (DEBUG, INFO, etc.)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"output_log.txt\"),  # Log to file\n",
    "        #logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llmsherpa_path = os.getenv('PATH_TO_LLMSHERPA')\n",
    "\n",
    "if llmsherpa_path:\n",
    "    sys.path.insert(0, llmsherpa_path)\n",
    "    print(f\"Added {llmsherpa_path} to sys.path\")\n",
    "else:\n",
    "    print(\"Environment variable PATH_TO_LLMSHERPA is not set.\")\n",
    "\n",
    "folder_path = \"../data\"  # Path to your folder containing PDFs\n",
    "CHROMA_PATH = os.getenv(\"CHROMA_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PARSER\n",
    "from llmsherpa.readers import LayoutPDFReader\n",
    "parser_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\" \n",
    "pdf_reader = LayoutPDFReader(parser_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(BaseDocument):\n",
    "    \"\"\"Class for storing a piece of text and associated metadata.\n",
    "    \n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "            document = Document(\n",
    "                page_content=\"Hello, world!\",\n",
    "                metadata={\"source\": \"https://example.com\", \"page_idx\": 1, \"block_idx\": 0, \"tag\": \"greeting\"}\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    page_content: str\n",
    "    \"\"\"String text.\"\"\"\n",
    "    type: Literal[\"Document\"] = \"Document\"\n",
    "\n",
    "    def __init__(self, page_content: str, metadata: Dict[str, Any] = None) -> None:\n",
    "        \"\"\"Initialize a Document instance.\n",
    "        \n",
    "        Args:\n",
    "            page_content (str): The content of the document.\n",
    "            metadata (dict): A dictionary containing metadata about the document.\n",
    "        \"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        super().__init__(page_content=page_content, metadata=metadata)  # Call the base class initializer\n",
    "\n",
    "    @classmethod\n",
    "    def is_lc_serializable(cls) -> bool:\n",
    "        \"\"\"Return whether this class is serializable.\"\"\"\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def get_lc_namespace(cls) -> List[str]:\n",
    "        \"\"\"Get the namespace of the langchain object.\"\"\"\n",
    "        return [\"langchain\", \"schema\", \"document\"]\n",
    "\n",
    "    def format_metadata(self) -> Dict[str, str]:\n",
    "        \"\"\"Format the metadata of the document.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing formatted metadata with predefined attributes.\n",
    "        \"\"\"\n",
    "        attributes = ['source', 'page_idx', 'block_idx', 'tag', 'type']\n",
    "        formatted_metadata = {key: self.metadata.get(key, 'unknown') for key in attributes}\n",
    "        return formatted_metadata\n",
    "    \n",
    "    @property\n",
    "    def source(self) -> str:\n",
    "        \"\"\"Get the source of the document.\"\"\"\n",
    "        return self.metadata.get('source', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def page_idx(self) -> str:\n",
    "        \"\"\"Get the page index of the document.\"\"\"\n",
    "        return self.metadata.get('page_idx', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def block_idx(self) -> str:\n",
    "        \"\"\"Get the block index of the document.\"\"\"\n",
    "        return self.metadata.get('block_idx', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def tag(self) -> str:\n",
    "        \"\"\"Get the tag of the document.\"\"\"\n",
    "        return self.metadata.get('tag', 'unknown')\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        \"\"\"Get the type of the document.\"\"\"\n",
    "        return self.metadata.get('type', 'unknown')\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Return a string representation of the Document instance.\"\"\"\n",
    "        metadata_str = ', '.join(f\"{key}={value}\" for key, value in self.format_metadata().items())\n",
    "        return f\"Document({metadata_str}, content={self.page_content})\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Override __str__ to restrict it to page_content and metadata.\"\"\"\n",
    "        if self.metadata:\n",
    "            return f\"page_content='{self.page_content}' metadata={self.metadata}\"\n",
    "        else:\n",
    "            return f\"page_content='{self.page_content}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path: str, pdf_reader: LayoutPDFReader) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process a PDF file and extract its content into Document instances.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file.\n",
    "        pdf_reader (LayoutPDFReader): An instance of LayoutPDFReader to read the PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document instances representing the content of the PDF.\n",
    "    \"\"\"\n",
    "    # Read PDF content\n",
    "    doc = pdf_reader.read_pdf(file_path)\n",
    "\n",
    "    documents = []\n",
    "    for chunk in doc.chunks():\n",
    "        # Extract metadata using chunk attributes\n",
    "        metadata = {\n",
    "            'source': os.path.basename(file_path),\n",
    "            'page_idx': chunk.page_idx,\n",
    "            'block_idx': chunk.block_idx,\n",
    "            'tag': chunk.tag,\n",
    "            'type': type(chunk).__name__  # Get the type of the chunk (e.g., 'Paragraph', 'Table', etc.)\n",
    "        }\n",
    "        # Extract text content from chunk\n",
    "        page_content = chunk.to_text()\n",
    "\n",
    "        # Create a Document instance\n",
    "        document = Document(metadata=metadata, page_content=page_content)\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path: str, pdf_reader: LayoutPDFReader) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process all PDF files in a folder and extract their content into Document instances.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing PDF files.\n",
    "        pdf_reader (LayoutPDFReader): An instance of LayoutPDFReader to read the PDFs.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document instances representing the content of all PDFs in the folder.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            try:\n",
    "                # Process each PDF file and extend the documents list\n",
    "                documents = process_pdf(file_path, pdf_reader)\n",
    "                all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    A class to handle the chunking of documents.\n",
    "\n",
    "    Attributes:\n",
    "        model (SentenceTransformer): The sentence transformer model used for embeddings.\n",
    "        splitter (RecursiveCharacterTextSplitter): The text splitter instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, chunk_size: int, chunk_overlap: int):\n",
    "        \"\"\"\n",
    "        Initialize a DocumentChunker instance.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the model for the tokenizer.\n",
    "            chunk_size (int): The maximum size of each chunk.\n",
    "            chunk_overlap (int): The number of overlapping tokens between chunks.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer=self.model.tokenizer,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chunk a list of Document instances.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): The list of Document instances to be chunked.\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: A list of Document instances representing the chunks.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chunks = self.splitter.split_documents(documents)\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error during chunking: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Calculate unique IDs for each chunk based on source, page number, and chunk index.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[Document]): A list of Document instances.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: The updated list of Document instances with calculated IDs.\n",
    "    \"\"\"\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\", \"unknown\")\n",
    "        page = chunk.metadata.get(\"page_idx\", \"unknown\")  # Use 'page_idx' based on your Document definition\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the chunk metadata.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbeddingFunction:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, input: Union[str, List[str]]) -> List[List[float]]:\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        embeddings = self.model.encode(input, show_progress_bar=False)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return self(documents)\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: List[Document], CHROMA_PATH: str) -> None:\n",
    "    \"\"\"\n",
    "    Add new document chunks to the Chroma database if they don't already exist.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[Document]): A list of Document instances to be added.\n",
    "        CHROMA_PATH (str): The path to the Chroma database.\n",
    "        hf: The embedding function for Hugging Face embeddings.\n",
    "    \"\"\"\n",
    "    # Load the existing database with Hugging Face embeddings\n",
    "    try:\n",
    "        embedding_function = TransformerEmbeddingFunction()\n",
    "        db = Chroma(\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "\n",
    "        # Calculate Page IDs.\n",
    "        chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "        # Add or Update the documents.\n",
    "        existing_items = db.get(include=[])  # IDs are always included by default\n",
    "        existing_ids = set(existing_items[\"ids\"])\n",
    "        print(f\"Number of existing doc segments in DB: {len(existing_ids)}\")\n",
    "\n",
    "        # Only add documents that don't exist in the DB.\n",
    "        new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "        if new_chunks:\n",
    "            print(f\"ðŸ‘‰ Adding new doc segments: {len(new_chunks)}\")\n",
    "            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "            db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "            print(f\"âœ… Successfully added {len(new_chunks)} new doc segments.\")\n",
    "        else:\n",
    "            print(\"âœ… No new doc segments to add\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error while adding to Chroma: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ShowcaseUserGuide.pdf\n",
      "Processing file: access_019-access-management-system-user-guide-v4-0.pdf\n",
      "Processing file: nihms-1769170.pdf\n",
      "Processing file: Bookshelf_NBK5295.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/paperrag-dev/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing doc segments in DB: 8793\n",
      "âœ… No new doc segments to add\n"
     ]
    }
   ],
   "source": [
    "all_docs = process_folder(folder_path, pdf_reader)\n",
    "chunker = DocumentChunker(model_name='sentence-transformers/all-MiniLM-L6-v2', chunk_size=1024, chunk_overlap=128)\n",
    "chunks = chunker.chunk_documents(all_docs)\n",
    "add_to_chroma(chunks, CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(pdf_directory: str, parser_api_url: str) -> List[BaseDocument]:\n",
    "    \"\"\"\n",
    "    Load documents from a directory of PDF files and convert them into Document instances.\n",
    "\n",
    "    Args:\n",
    "        pdf_directory (str): The directory containing the PDF files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document instances created from the PDF files.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(pdf_directory, pdf_file)\n",
    "        # Process the PDF file and extract content into Document instances\n",
    "        doc_instances = process_pdf(file_path, pdf_reader=LayoutPDFReader(parser_api_url=parser_api_url))\n",
    "        documents.extend(doc_instances)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaRetrieval:\n",
    "    def __init__(self, CHROMA_PATH: str, documents: List[BaseDocument], num_queries: int):\n",
    "        self.embedding_function = TransformerEmbeddingFunction()\n",
    "        self.db = Chroma(\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "        \n",
    "        # Load documents into the vector database\n",
    "        self.db.add_documents(documents)\n",
    "        \n",
    "        # Set up the retrievers\n",
    "        self.vector_retriever = self.db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": num_queries})\n",
    "        self.keyword_retriever = BM25Retriever.from_texts([doc.page_content for doc in documents])\n",
    "\n",
    "        self.keyword_retriever.k = num_queries  # Set the number of top documents to return\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "\n",
    "        # Initialize the ensemble retriever\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.keyword_retriever, self.vector_retriever],\n",
    "            weights=[0.8, 0.2]  # Adjust weights as necessary\n",
    "        )\n",
    "\n",
    "        # Use the ensemble retriever to get results\n",
    "        ensemble_results = ensemble_retriever.invoke(query)\n",
    "\n",
    "        # Combine results with additional data\n",
    "        combined_results = []\n",
    "        for result in ensemble_results:\n",
    "            chunk_data = {\n",
    "                \"score\": result.metadata.get(\"score\", 0),  # Adjust this as needed\n",
    "                \"document\": result,\n",
    "                \"id\": result.metadata.get(\"id\", \"unknown\"),\n",
    "                \"source\": result.metadata.get(\"source\", \"unknown\"),\n",
    "                \"type\": \"combined\"  # You can label it as \"combined\" or something else\n",
    "            }\n",
    "            combined_results.append(chunk_data)\n",
    "\n",
    "        # Sort combined results by score if necessary\n",
    "        combined_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        return combined_results[:top_k] \n",
    "\n",
    "    def generate_response(self, query_text: str, llm_model_name: str, prompt_template: str, top_k: int = 5) -> str:\n",
    "        try:\n",
    "            # Retrieve results using the query\n",
    "            results = self.retrieve(query_text, top_k=top_k)\n",
    "            logging.debug(f\"Retrieved results: {results}\")\n",
    "            \n",
    "            # Combine the results into a single context text\n",
    "            context_text = \"\\n\\n---\\n\\n\".join([doc['document'].page_content for doc in results])\n",
    "            logging.debug(f\"Combined context text: {context_text}\")\n",
    "\n",
    "            # Create the prompt using the provided template\n",
    "            prompt_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "            prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "            logging.debug(f\"Formatted prompt: {prompt}\")\n",
    "            \n",
    "\n",
    "            # Measure the time taken to generate the response\n",
    "            start_time = time.time()\n",
    "            model = Ollama(model=llm_model_name, format=\"json\")\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            logging.info(f\"{llm_model_name} model initialization time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "            inference_start_time = time.time()\n",
    "            response_text = model.invoke(prompt)\n",
    "            inference_end_time = time.time()\n",
    "            elapsed_time = inference_end_time - inference_start_time\n",
    "            logging.info(f\"{llm_model_name} inference time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "            # Parse the response if necessary\n",
    "            try:\n",
    "                response_json = json.loads(response_text)\n",
    "                logging.debug(f\"Parsed response: {response_json}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"Failed to parse response as JSON: {e}\")\n",
    "                response_json = {\"error\": \"Failed to parse response\"}\n",
    "            \n",
    "            return response_text #might not need load_json but good to check\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating response: {e}\")\n",
    "            return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 19:53:51,116 - DEBUG - Starting new HTTP connection (1): localhost:5010\n",
      "2024-09-24 19:53:52,023 - DEBUG - http://localhost:5010 \"POST /api/parseDocument?renderFormat=all HTTP/11\" 200 0\n",
      "2024-09-24 19:53:52,028 - DEBUG - Starting new HTTP connection (1): localhost:5010\n",
      "2024-09-24 19:53:53,528 - DEBUG - http://localhost:5010 \"POST /api/parseDocument?renderFormat=all HTTP/11\" 200 0\n",
      "2024-09-24 19:53:53,535 - DEBUG - Starting new HTTP connection (1): localhost:5010\n",
      "2024-09-24 19:53:55,301 - DEBUG - http://localhost:5010 \"POST /api/parseDocument?renderFormat=all HTTP/11\" 200 0\n",
      "2024-09-24 19:53:55,309 - DEBUG - Starting new HTTP connection (1): localhost:5010\n",
      "2024-09-24 19:53:58,692 - DEBUG - http://localhost:5010 \"POST /api/parseDocument?renderFormat=all HTTP/11\" 200 0\n",
      "2024-09-24 19:53:58,707 - INFO - Use pytorch device_name: mps\n",
      "2024-09-24 19:53:58,709 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2024-09-24 19:53:58,834 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/11\" 200 0\n",
      "2024-09-24 19:53:58,962 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/11\" 200 0\n",
      "2024-09-24 19:53:59,131 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/11\" 200 0\n",
      "2024-09-24 19:53:59,259 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/11\" 200 0\n",
      "2024-09-24 19:53:59,384 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/11\" 200 0\n",
      "2024-09-24 19:53:59,547 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/11\" 200 0\n",
      "2024-09-24 19:53:59,714 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "2024-09-24 19:53:59,930 - DEBUG - https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/11\" 200 6159\n",
      "2024-09-24 19:54:00,078 - DEBUG - https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/11\" 200 6159\n",
      "2024-09-24 19:54:00,973 - DEBUG - Collection langchain already exists, returning existing collection.\n",
      "2024-09-24 19:54:01,231 - DEBUG - Resetting dropped connection: us.i.posthog.com\n",
      "2024-09-24 19:54:01,705 - DEBUG - https://us.i.posthog.com:443 \"POST /batch/ HTTP/11\" 200 15\n"
     ]
    }
   ],
   "source": [
    "# Load documents from the specified directory\n",
    "documents = load_documents(folder_path, parser_api_url)\n",
    "\n",
    "chroma_retrieval = ChromaRetrieval(\n",
    "    CHROMA_PATH=CHROMA_PATH,\n",
    "    documents=documents,\n",
    "    num_queries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE RESPONSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query and prompt template\n",
    "prompt_template = \"\"\"\n",
    "    Context: \n",
    "    {context}\n",
    "\n",
    "    Question: \n",
    "    {question}\n",
    "\n",
    "    Instructions:\n",
    "    - Use the provided context to answer the question.\n",
    "    - Be as thorough and precise as possible.\n",
    "    - If the context does not provide enough information to answer the question, state that clearly.\n",
    "    - If relevant, also add organism information\n",
    "\n",
    "    Example JSON output:\n",
    "    {{\n",
    "        \"question\": \"What is the recommended input mass of stool for the DNA extraction protocol?\",\n",
    "        \"organism\": \"Human\"\n",
    "        \"context\": \"Keeping the frozen stool sample on dry ice as much as possible (to maintain sample integrity), place the sample tube in a tube rack and use a biopsy punch to distribute a 150-mg aliquot of stool into a 2-ml microcentrifuge tube.\",\n",
    "        \"answer\": \"150 mg\"\n",
    "    }}\n",
    "\n",
    "    Answer only in valid JSON format:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 19:54:42,969 - DEBUG - Retrieved results: [{'score': 0, 'document': Document(page_content='Experimental design Sample lysis and contaminant digestion (Steps 1â€“7)â€”We recommend using samples that have been divided into aliquots and frozen as soon after collection as possible, because extended time at room temperature before freezing can lead to altered microbial abundance, as some taxa may die while others continue to divide17,18.\\nWhen dividing stool samples into aliquots for DNA extraction, we prefer to use biopsy punches with plungers, which can precisely produce frozen stool aliquots and limit freeze-thaw cycles.\\nCare should be taken to avoid injury when using these sharp tools, and we recommend placing the sample tube in a rack rather than holding the tube by hand during the punching procedure.\\nWhen preparing aliquots, one should consider the biomass of the sample.\\nIf a stool sample has a lower biomass and is more watery in consistency, a greater total mass is recommended for extraction input.\\nAlternatively, multiple extractions can be performed in parallel and pooled on the column purification step (Step 9).'), 'id': 'unknown', 'source': 'unknown', 'type': 'combined'}, {'score': 0, 'document': Document(page_content='Size selection (Steps 13â€“15)â€”After DNA has been extracted, we recommend additional size selection to deplete shorter DNA fragments and enrich for longer fragments, as long fragments and their resulting long reads are critical for assembling contiguous genomes.\\nThere are several methodological choices for additional size selection, including size selection with the Sage Science BluePippin, the Circulomics Short Read Eliminator kit or Solid Phase Reversible Immobilization (SPRI) beads.\\nAlthough BluePippin size selection is effective at accurately and thoroughly eliminating DNA below a desired threshold, we find that the total mass lost with this protocol is high, necessitating a higher input mass to ensure adequate yield.\\nThe Circulomics Short Read Eliminator kit has not been evaluated in the context of this extraction protocol, but it is commonly used in HMW DNA extraction protocols and is worth consideration here.\\nFor applications where input sample is limited, we recommend size selection with SPRI beads, as they provide further sample purification and reasonable yield, and the supernatant can be retained for additional selection steps.\\nTypically, SPRI beads are used for sample clean-up and size selection.\\nAs the ratio of beads to sample is increased, the binding of smaller fragments to the beads becomes more efficient.\\nConversely, a lower ratio of beads to sample will lead to more stringent selection for longer DNA fragments.\\nHowever, as standard SPRI beads are typically intended for size selection within a range of 150â€“800 bp, preparation of the SPRI beads for fragment selection >2.5 kb requires a custom buffer, as detailed in Materials.\\nGiven the variable nature of a custom buffer preparation, a range of bead-to-sample volume ratios should be tested with a non-precious DNA sample, such as DNA extracted from an abundant stool sample, to determine an appropriate bead-to-sample ratio to achieve peak DNA fragment lengths of >15 kb and minimal mass <2.5 kb.\\nAfter size selection and an initial size distribution quantification with an Agilent TapeStation (see DNA quality assessment), additional rounds of size selection can be performed with lower bead-to-sample ratios to increase selection stringency if quantification shows retention of fragments below 2.5 kb.\\nConversely, the supernatant of each selection step can be retained for repooling in the event that the original selection was too stringent.\\nThe nuclease-free water volume for the final bead resuspension step can be altered to yield the desired input volume for downstream sequencing applications.\\nWe recommend resuspending the sample in 50 Î¼l for downstream library preparation with the Oxford Nanopore Genomic DNA by Ligation kit.\\nIt is important to note that size selection may lead to bias in organismal relative abundance, as organisms that lyse more easily may have smaller DNA fragment sizes and be selected against during size selection.'), 'id': 'unknown', 'source': 'unknown', 'type': 'combined'}, {'score': 0, 'document': Document(page_content='Advantages and limitationsâ€”This DNA extraction approach has been optimized for extraction of HMW DNA from human stool samples but has also been validated on mock microbial communities and bacterial isolates.\\nWe expect that this protocol can be adapted to alternative sample types, although modifications to pre- and post-lysis steps will probably be necessary to account for sample-specific preparation and contaminant cleanup.\\nMechanical lysis approaches such as bead beating remain the gold standard for consistent lysis and downstream relative-abundance classification, as mechanical lysis is considered less biased than enzymatic approaches.\\nHowever, we have previously shown that this enzymatic approach is capable of relatively consistent lysis from both Gram-positive and Gram-negative organisms10.\\nIn addition, although mechanical lysis approaches may be sufficient in circumstances with abundant input material, such that extensive size selection is allowable, our approach optimizes for high DNA yield and is advantageous in scenarios with limited input sample volume.'), 'id': 'unknown', 'source': 'unknown', 'type': 'combined'}, {'score': 0, 'document': Document(page_content='Library preparation and sequencing (Steps 19â€“22)â€”Although we generally recommend sequencing on the Oxford Nanopore MinION (or equivalent) platform because of its lower equipment costs, portability and ability to generate extremely long reads, the PacBio platform offers an excellent alternative in settings where researchers have access to a PacBio sequencer.\\nAlthough generally quite comparable to one another, one potential advantage of the PacBio platform over the Oxford Nanopore MinION is that the PacBio system yields reads with relatively random error profiles, which are more easily corrected in assembly when compared to the frequent homopolymer errors of the Oxford Nanopore MinION platform12,13.\\nFor the purpose of this protocol, we recommend preparing DNA for nanopore sequencing by using the Oxford Nanopore Genomic DNA by Ligation library preparation kit, which incorporates steps for DNA repair, DNA end preparation and sequencing adapter attachment.\\nThis protocol is intended for direct DNA sequencing, and we have found that it can yield up to 30 Gbp of sequencing from a single MinION R9.4 flow cell.\\nIn addition, this protocol includes AMPure bead cleanup steps that improve sample purity before sample loading.\\nTo ensure maximum yield and an optimized ratio of available DNA ends to sequencing adapters, the input DNA should be adjusted on the basis of the peak size and total mass to 100â€“200 fmol, as instructed in the extended Genomic DNA by Ligation protocol.\\nThe Oxford Nanopore Rapid Sequencing protocol is an acceptable alternative protocol for DNA extractions with lower total mass, as the protocolâ€™s suggested input is 400 ng.\\nThis protocol can be performed in 10 min and uses transposome-mediated tagmentation to attach sequencing adapters to DNA.\\nHowever, we find that libraries prepared with the Rapid Sequencing protocol have a lower yield of total sequencing data than do libraries prepared with 300â€“400 ng of input DNA by using the Genomic DNA by Ligation protocol.'), 'id': 'unknown', 'source': 'unknown', 'type': 'combined'}, {'score': 0, 'document': Document(page_content='This protocol describes methods for extraction, sequencing, assembly and binning of HMW DNA from human stool samples.\\nIn our experience, we find that the DNA extraction method described here can yield 1â€“2 Î¼g of DNA from an initial input of 300â€“500 mg of stool.\\nThis DNA has a size distribution peak of 15â€“50 kb, which is sufficient for library preparation without PCR amplification and subsequent sequencing on an Oxford Nanopore MinION sequencer (Supplementary Fig. 1).\\nWe find that these methods are capable of generating 6â€“30 Gbp of long-read data on MinION R9.4 flow cells.\\nIn our experience, the Lathe workflow is capable of producing at least one circular bacterial genome from a complex gut metagenome with 6 Gbp of long-read data.\\nHowever, these results may vary with coverage, gut complexity, DNA fragment size and bacterial genomic structure.'), 'id': 'unknown', 'source': 'unknown', 'type': 'combined'}]\n",
      "2024-09-24 19:54:42,970 - DEBUG - Combined context text: Experimental design Sample lysis and contaminant digestion (Steps 1â€“7)â€”We recommend using samples that have been divided into aliquots and frozen as soon after collection as possible, because extended time at room temperature before freezing can lead to altered microbial abundance, as some taxa may die while others continue to divide17,18.\n",
      "When dividing stool samples into aliquots for DNA extraction, we prefer to use biopsy punches with plungers, which can precisely produce frozen stool aliquots and limit freeze-thaw cycles.\n",
      "Care should be taken to avoid injury when using these sharp tools, and we recommend placing the sample tube in a rack rather than holding the tube by hand during the punching procedure.\n",
      "When preparing aliquots, one should consider the biomass of the sample.\n",
      "If a stool sample has a lower biomass and is more watery in consistency, a greater total mass is recommended for extraction input.\n",
      "Alternatively, multiple extractions can be performed in parallel and pooled on the column purification step (Step 9).\n",
      "\n",
      "---\n",
      "\n",
      "Size selection (Steps 13â€“15)â€”After DNA has been extracted, we recommend additional size selection to deplete shorter DNA fragments and enrich for longer fragments, as long fragments and their resulting long reads are critical for assembling contiguous genomes.\n",
      "There are several methodological choices for additional size selection, including size selection with the Sage Science BluePippin, the Circulomics Short Read Eliminator kit or Solid Phase Reversible Immobilization (SPRI) beads.\n",
      "Although BluePippin size selection is effective at accurately and thoroughly eliminating DNA below a desired threshold, we find that the total mass lost with this protocol is high, necessitating a higher input mass to ensure adequate yield.\n",
      "The Circulomics Short Read Eliminator kit has not been evaluated in the context of this extraction protocol, but it is commonly used in HMW DNA extraction protocols and is worth consideration here.\n",
      "For applications where input sample is limited, we recommend size selection with SPRI beads, as they provide further sample purification and reasonable yield, and the supernatant can be retained for additional selection steps.\n",
      "Typically, SPRI beads are used for sample clean-up and size selection.\n",
      "As the ratio of beads to sample is increased, the binding of smaller fragments to the beads becomes more efficient.\n",
      "Conversely, a lower ratio of beads to sample will lead to more stringent selection for longer DNA fragments.\n",
      "However, as standard SPRI beads are typically intended for size selection within a range of 150â€“800 bp, preparation of the SPRI beads for fragment selection >2.5 kb requires a custom buffer, as detailed in Materials.\n",
      "Given the variable nature of a custom buffer preparation, a range of bead-to-sample volume ratios should be tested with a non-precious DNA sample, such as DNA extracted from an abundant stool sample, to determine an appropriate bead-to-sample ratio to achieve peak DNA fragment lengths of >15 kb and minimal mass <2.5 kb.\n",
      "After size selection and an initial size distribution quantification with an Agilent TapeStation (see DNA quality assessment), additional rounds of size selection can be performed with lower bead-to-sample ratios to increase selection stringency if quantification shows retention of fragments below 2.5 kb.\n",
      "Conversely, the supernatant of each selection step can be retained for repooling in the event that the original selection was too stringent.\n",
      "The nuclease-free water volume for the final bead resuspension step can be altered to yield the desired input volume for downstream sequencing applications.\n",
      "We recommend resuspending the sample in 50 Î¼l for downstream library preparation with the Oxford Nanopore Genomic DNA by Ligation kit.\n",
      "It is important to note that size selection may lead to bias in organismal relative abundance, as organisms that lyse more easily may have smaller DNA fragment sizes and be selected against during size selection.\n",
      "\n",
      "---\n",
      "\n",
      "Advantages and limitationsâ€”This DNA extraction approach has been optimized for extraction of HMW DNA from human stool samples but has also been validated on mock microbial communities and bacterial isolates.\n",
      "We expect that this protocol can be adapted to alternative sample types, although modifications to pre- and post-lysis steps will probably be necessary to account for sample-specific preparation and contaminant cleanup.\n",
      "Mechanical lysis approaches such as bead beating remain the gold standard for consistent lysis and downstream relative-abundance classification, as mechanical lysis is considered less biased than enzymatic approaches.\n",
      "However, we have previously shown that this enzymatic approach is capable of relatively consistent lysis from both Gram-positive and Gram-negative organisms10.\n",
      "In addition, although mechanical lysis approaches may be sufficient in circumstances with abundant input material, such that extensive size selection is allowable, our approach optimizes for high DNA yield and is advantageous in scenarios with limited input sample volume.\n",
      "\n",
      "---\n",
      "\n",
      "Library preparation and sequencing (Steps 19â€“22)â€”Although we generally recommend sequencing on the Oxford Nanopore MinION (or equivalent) platform because of its lower equipment costs, portability and ability to generate extremely long reads, the PacBio platform offers an excellent alternative in settings where researchers have access to a PacBio sequencer.\n",
      "Although generally quite comparable to one another, one potential advantage of the PacBio platform over the Oxford Nanopore MinION is that the PacBio system yields reads with relatively random error profiles, which are more easily corrected in assembly when compared to the frequent homopolymer errors of the Oxford Nanopore MinION platform12,13.\n",
      "For the purpose of this protocol, we recommend preparing DNA for nanopore sequencing by using the Oxford Nanopore Genomic DNA by Ligation library preparation kit, which incorporates steps for DNA repair, DNA end preparation and sequencing adapter attachment.\n",
      "This protocol is intended for direct DNA sequencing, and we have found that it can yield up to 30 Gbp of sequencing from a single MinION R9.4 flow cell.\n",
      "In addition, this protocol includes AMPure bead cleanup steps that improve sample purity before sample loading.\n",
      "To ensure maximum yield and an optimized ratio of available DNA ends to sequencing adapters, the input DNA should be adjusted on the basis of the peak size and total mass to 100â€“200 fmol, as instructed in the extended Genomic DNA by Ligation protocol.\n",
      "The Oxford Nanopore Rapid Sequencing protocol is an acceptable alternative protocol for DNA extractions with lower total mass, as the protocolâ€™s suggested input is 400 ng.\n",
      "This protocol can be performed in 10 min and uses transposome-mediated tagmentation to attach sequencing adapters to DNA.\n",
      "However, we find that libraries prepared with the Rapid Sequencing protocol have a lower yield of total sequencing data than do libraries prepared with 300â€“400 ng of input DNA by using the Genomic DNA by Ligation protocol.\n",
      "\n",
      "---\n",
      "\n",
      "This protocol describes methods for extraction, sequencing, assembly and binning of HMW DNA from human stool samples.\n",
      "In our experience, we find that the DNA extraction method described here can yield 1â€“2 Î¼g of DNA from an initial input of 300â€“500 mg of stool.\n",
      "This DNA has a size distribution peak of 15â€“50 kb, which is sufficient for library preparation without PCR amplification and subsequent sequencing on an Oxford Nanopore MinION sequencer (Supplementary Fig. 1).\n",
      "We find that these methods are capable of generating 6â€“30 Gbp of long-read data on MinION R9.4 flow cells.\n",
      "In our experience, the Lathe workflow is capable of producing at least one circular bacterial genome from a complex gut metagenome with 6 Gbp of long-read data.\n",
      "However, these results may vary with coverage, gut complexity, DNA fragment size and bacterial genomic structure.\n",
      "2024-09-24 19:54:42,971 - DEBUG - Formatted prompt: Human: \n",
      "    Context: \n",
      "    Experimental design Sample lysis and contaminant digestion (Steps 1â€“7)â€”We recommend using samples that have been divided into aliquots and frozen as soon after collection as possible, because extended time at room temperature before freezing can lead to altered microbial abundance, as some taxa may die while others continue to divide17,18.\n",
      "When dividing stool samples into aliquots for DNA extraction, we prefer to use biopsy punches with plungers, which can precisely produce frozen stool aliquots and limit freeze-thaw cycles.\n",
      "Care should be taken to avoid injury when using these sharp tools, and we recommend placing the sample tube in a rack rather than holding the tube by hand during the punching procedure.\n",
      "When preparing aliquots, one should consider the biomass of the sample.\n",
      "If a stool sample has a lower biomass and is more watery in consistency, a greater total mass is recommended for extraction input.\n",
      "Alternatively, multiple extractions can be performed in parallel and pooled on the column purification step (Step 9).\n",
      "\n",
      "---\n",
      "\n",
      "Size selection (Steps 13â€“15)â€”After DNA has been extracted, we recommend additional size selection to deplete shorter DNA fragments and enrich for longer fragments, as long fragments and their resulting long reads are critical for assembling contiguous genomes.\n",
      "There are several methodological choices for additional size selection, including size selection with the Sage Science BluePippin, the Circulomics Short Read Eliminator kit or Solid Phase Reversible Immobilization (SPRI) beads.\n",
      "Although BluePippin size selection is effective at accurately and thoroughly eliminating DNA below a desired threshold, we find that the total mass lost with this protocol is high, necessitating a higher input mass to ensure adequate yield.\n",
      "The Circulomics Short Read Eliminator kit has not been evaluated in the context of this extraction protocol, but it is commonly used in HMW DNA extraction protocols and is worth consideration here.\n",
      "For applications where input sample is limited, we recommend size selection with SPRI beads, as they provide further sample purification and reasonable yield, and the supernatant can be retained for additional selection steps.\n",
      "Typically, SPRI beads are used for sample clean-up and size selection.\n",
      "As the ratio of beads to sample is increased, the binding of smaller fragments to the beads becomes more efficient.\n",
      "Conversely, a lower ratio of beads to sample will lead to more stringent selection for longer DNA fragments.\n",
      "However, as standard SPRI beads are typically intended for size selection within a range of 150â€“800 bp, preparation of the SPRI beads for fragment selection >2.5 kb requires a custom buffer, as detailed in Materials.\n",
      "Given the variable nature of a custom buffer preparation, a range of bead-to-sample volume ratios should be tested with a non-precious DNA sample, such as DNA extracted from an abundant stool sample, to determine an appropriate bead-to-sample ratio to achieve peak DNA fragment lengths of >15 kb and minimal mass <2.5 kb.\n",
      "After size selection and an initial size distribution quantification with an Agilent TapeStation (see DNA quality assessment), additional rounds of size selection can be performed with lower bead-to-sample ratios to increase selection stringency if quantification shows retention of fragments below 2.5 kb.\n",
      "Conversely, the supernatant of each selection step can be retained for repooling in the event that the original selection was too stringent.\n",
      "The nuclease-free water volume for the final bead resuspension step can be altered to yield the desired input volume for downstream sequencing applications.\n",
      "We recommend resuspending the sample in 50 Î¼l for downstream library preparation with the Oxford Nanopore Genomic DNA by Ligation kit.\n",
      "It is important to note that size selection may lead to bias in organismal relative abundance, as organisms that lyse more easily may have smaller DNA fragment sizes and be selected against during size selection.\n",
      "\n",
      "---\n",
      "\n",
      "Advantages and limitationsâ€”This DNA extraction approach has been optimized for extraction of HMW DNA from human stool samples but has also been validated on mock microbial communities and bacterial isolates.\n",
      "We expect that this protocol can be adapted to alternative sample types, although modifications to pre- and post-lysis steps will probably be necessary to account for sample-specific preparation and contaminant cleanup.\n",
      "Mechanical lysis approaches such as bead beating remain the gold standard for consistent lysis and downstream relative-abundance classification, as mechanical lysis is considered less biased than enzymatic approaches.\n",
      "However, we have previously shown that this enzymatic approach is capable of relatively consistent lysis from both Gram-positive and Gram-negative organisms10.\n",
      "In addition, although mechanical lysis approaches may be sufficient in circumstances with abundant input material, such that extensive size selection is allowable, our approach optimizes for high DNA yield and is advantageous in scenarios with limited input sample volume.\n",
      "\n",
      "---\n",
      "\n",
      "Library preparation and sequencing (Steps 19â€“22)â€”Although we generally recommend sequencing on the Oxford Nanopore MinION (or equivalent) platform because of its lower equipment costs, portability and ability to generate extremely long reads, the PacBio platform offers an excellent alternative in settings where researchers have access to a PacBio sequencer.\n",
      "Although generally quite comparable to one another, one potential advantage of the PacBio platform over the Oxford Nanopore MinION is that the PacBio system yields reads with relatively random error profiles, which are more easily corrected in assembly when compared to the frequent homopolymer errors of the Oxford Nanopore MinION platform12,13.\n",
      "For the purpose of this protocol, we recommend preparing DNA for nanopore sequencing by using the Oxford Nanopore Genomic DNA by Ligation library preparation kit, which incorporates steps for DNA repair, DNA end preparation and sequencing adapter attachment.\n",
      "This protocol is intended for direct DNA sequencing, and we have found that it can yield up to 30 Gbp of sequencing from a single MinION R9.4 flow cell.\n",
      "In addition, this protocol includes AMPure bead cleanup steps that improve sample purity before sample loading.\n",
      "To ensure maximum yield and an optimized ratio of available DNA ends to sequencing adapters, the input DNA should be adjusted on the basis of the peak size and total mass to 100â€“200 fmol, as instructed in the extended Genomic DNA by Ligation protocol.\n",
      "The Oxford Nanopore Rapid Sequencing protocol is an acceptable alternative protocol for DNA extractions with lower total mass, as the protocolâ€™s suggested input is 400 ng.\n",
      "This protocol can be performed in 10 min and uses transposome-mediated tagmentation to attach sequencing adapters to DNA.\n",
      "However, we find that libraries prepared with the Rapid Sequencing protocol have a lower yield of total sequencing data than do libraries prepared with 300â€“400 ng of input DNA by using the Genomic DNA by Ligation protocol.\n",
      "\n",
      "---\n",
      "\n",
      "This protocol describes methods for extraction, sequencing, assembly and binning of HMW DNA from human stool samples.\n",
      "In our experience, we find that the DNA extraction method described here can yield 1â€“2 Î¼g of DNA from an initial input of 300â€“500 mg of stool.\n",
      "This DNA has a size distribution peak of 15â€“50 kb, which is sufficient for library preparation without PCR amplification and subsequent sequencing on an Oxford Nanopore MinION sequencer (Supplementary Fig. 1).\n",
      "We find that these methods are capable of generating 6â€“30 Gbp of long-read data on MinION R9.4 flow cells.\n",
      "In our experience, the Lathe workflow is capable of producing at least one circular bacterial genome from a complex gut metagenome with 6 Gbp of long-read data.\n",
      "However, these results may vary with coverage, gut complexity, DNA fragment size and bacterial genomic structure.\n",
      "\n",
      "    Question: \n",
      "    What is the recommended input mass of stool for the DNA extraction protocol?\n",
      "\n",
      "    Instructions:\n",
      "    - Use the provided context to answer the question.\n",
      "    - Be as thorough and precise as possible.\n",
      "    - If the context does not provide enough information to answer the question, state that clearly.\n",
      "    - If relevant, also add organism information\n",
      "\n",
      "    Example JSON output:\n",
      "    {\n",
      "        \"question\": \"What is the recommended input mass of stool for the DNA extraction protocol?\",\n",
      "        \"organism\": \"Human\"\n",
      "        \"context\": \"Keeping the frozen stool sample on dry ice as much as possible (to maintain sample integrity), place the sample tube in a tube rack and use a biopsy punch to distribute a 150-mg aliquot of stool into a 2-ml microcentrifuge tube.\",\n",
      "        \"answer\": \"150 mg\"\n",
      "    }\n",
      "\n",
      "    Answer only in valid JSON format:\n",
      "\n",
      "2024-09-24 19:54:42,972 - INFO - llama3.1 model initialization time: 0.00 seconds\n",
      "2024-09-24 19:54:42,975 - DEBUG - Starting new HTTP connection (1): localhost:11434\n",
      "2024-09-24 19:57:06,754 - DEBUG - http://localhost:11434 \"POST /api/generate HTTP/11\" 200 None\n",
      "2024-09-24 19:57:27,828 - INFO - llama3.1 inference time: 164.86 seconds\n",
      "2024-09-24 19:57:27,829 - DEBUG - Parsed response: {'question': 'What is the recommended input mass of stool for the DNA extraction protocol?', 'organism': 'Human', 'context': 'When preparing aliquots, one should consider the biomass of the sample. If a stool sample has a lower biomass and is more watery in consistency, a greater total mass is recommended for extraction input.', 'answer': 'A greater total mass (no specific amount mentioned) for samples with lower biomass and are more watery in consistency'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the recommended input mass of stool for the DNA extraction protocol?', 'organism': 'Human', 'context': 'When preparing aliquots, one should consider the biomass of the sample. If a stool sample has a lower biomass and is more watery in consistency, a greater total mass is recommended for extraction input.', 'answer': 'A greater total mass (no specific amount mentioned) for samples with lower biomass and are more watery in consistency'}\n"
     ]
    }
   ],
   "source": [
    "# Generate response to a test question\n",
    "query_text = \"What is the recommended input mass of stool for the DNA extraction protocol?\"\n",
    "response_text = chroma_retrieval.generate_response(\n",
    "    query_text=query_text,\n",
    "    llm_model_name=\"llama3.1\",\n",
    "    prompt_template=prompt_template\n",
    ")\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1 response generation time: 122.15 seconds\n",
      "{\n",
      "    \"question\": \"Which enzymes are used in the enzymatic lysis step?\",\n",
      "    \"context\": \"In general, we recommend enzymatic lysis using a combination of lytic enzyme solution (Qiagen 158928) and MetaPolyzyme (Millipore Sigma MAC4L-5MG) for effective lysis of a range of microbes with minimal shearing.\",\n",
      "    \"answer\": {\n",
      "        \"lyticase and chitinase\": \"disrupt glucan and chitin in cell walls\",\n",
      "        \"lysozyme, mutanolysin and lysostaphin\": \"disrupt linkages in peptidoglycans\",\n",
      "        \"achromopeptidase\": \"a lysyl endopeptidase that is effective in lysing Gram-positive bacteria\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Generate response to a test question\n",
    "query_text = \"Which enzymes are used in the enzymatic lysis step?\"\n",
    "response_text = chroma_retrieval.generate_response(\n",
    "    query_text=query_text,\n",
    "    llm_model_name=\"llama3.1\",\n",
    "    prompt_template=prompt_template\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User the  Docker Image of Ollama improve the inference time by utilizing GPU (https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Improve the query function\n",
    "- Prompt Engineering\n",
    "\n",
    "- async ops for DB generation\n",
    "\n",
    "- Knowledge graph integration \n",
    "\n",
    "- Multi-Modality with Llava embeddings\n",
    "\n",
    "- GUI: Streamlit (2 step installation with Docker?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperrag-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
